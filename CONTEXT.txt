Job Role: Data Engineer

job_description: Role Data Engineer

Location â€“ Woonsocket RI

Mandatory Technical / Functional Skills Strong skills in Python Programming

Experience with Pyspark and Hive
Strong Experience in SQL
Experience with data pipeline and workflow management tool: Airflow, Azure DataFlow etc. DataBricks Service (on any cloud, preferably Azure)
Experience with cloud services & Data Analytics tools : Databricks, snowflake, Azure ADLS Gen2, Azure/GCP
Pandas Dataframes and UDF knowledge
Snowflake Experience (may not be expert)
To work on data engineering solutions to leverage best practices and industry-leading tools and technologies to profile data, develop efficient ingestion, and build semantic data layers

Roles and responsibility

Support analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Support and monitor the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Pyspark and Azure â€˜big dataâ€™ technologies.
Work with data and analytics experts to strive for greater functionality in our data systems.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Developing, supporting and monitoring â€˜big dataâ€™ data pipelines.
Work closely with client to discover needs, collect appropriate data, and deliver valuable data products.
Provide valuable insights from data sets quickly.
Communicate complex findings and ideas in plain language and visualizations that is friendly to business and operational audiences.

Skills-

Azure, Databricks, snowflake, Azure Data lake Gen2, Python, PySpark, SQL, Orchestration tools like Airflow
Retail (Pharmacy, Healthcare business) domain Experience is added advantage. Excellent communication skill

Total experience in required skill (years) Experience : 5-7 years

Voir plus

Answered Questions:
    1. Role Definition: The role of a Data Engineer in this organization involves working on data engineering solutions, leveraging best practices and industry-leading tools and technologies to profile data, develop efficient ingestion, and build semantic data layers. The role also involves supporting and monitoring the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Pyspark and Azure ‘big data’ technologies.
    2. Key Responsibilities: The top responsibilities for this role include: 
        - Supporting analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
        - Working with data and analytics experts to strive for greater functionality in our data systems.
        - Working with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
    3. Experience: The essential experiences for this role include: 
        - Strong skills in Python Programming
        - Experience with Pyspark and Hive
        - Strong Experience in SQL
        - Experience with data pipeline and workflow management tool: Airflow, Azure DataFlow etc.
        - Experience with cloud services & Data Analytics tools : Databricks, snowflake, Azure ADLS Gen2, Azure/GCP
    4. Education and Certifications: Not Provided
    5. Must have skills: The essential skills for this role include: 
        - Python Programming
        - Pyspark and Hive
        - SQL
        - Data pipeline and workflow management tool: Airflow, Azure DataFlow etc.
    6. Good-to-have skills: The preferred skills for this role include: 
        - Experience with Databricks, snowflake, Azure ADLS Gen2, Azure/GCP
        - Retail (Pharmacy, Healthcare business) domain Experience
    7. Growth Opportunities: Not Provided
    8. Tools and Software: The candidate should be familiar with tools and software such as Azure, Databricks, snowflake, Azure Data lake Gen2, Python, PySpark, SQL, Orchestration tools like Airflow.
    9. Budget and Compensation: Not Provided
    10. Technical Job Functions: The technical job functions related to this role include developing, supporting and monitoring ‘big data’ data pipelines.
    11. Project and Task Management: Not Provided
    12. Seniority Level: The seniority level of this role is Mid-level, hands-on, based on the required experience of 5-7 years.
    13. Work Environment: Not Provided
    14. Acceptable Work Location/s: The acceptable work location is Woonsocket RI.
    15. Specific Industry: The role belongs to the Retail (Pharmacy, Healthcare business) industry.

