{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import tqdm\n",
    "\n",
    "pinecone.init(      \n",
    "\tapi_key='2906808a-8423-49a6-a6ed-554d7094d82c',      \n",
    "\tenvironment='gcp-starter'      \n",
    ")      \n",
    "index = pinecone.Index('jd-builder')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"C:\\\\Users\\\\abuza\\\\Desktop\\\\jobd\\\\api.env\")\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "response = openai.Embedding.create(\n",
    "  input=\"Educative answers section is helpful\",\n",
    "  model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "print(len(response['data'][0]['embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLOps Engineer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "def read_and_separate_roles(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "    \n",
    "    # Split the data into individual roles based on two newline characters\n",
    "    role = data.split('\\n\\n')\n",
    "    return role\n",
    "\n",
    "def make_embeddings(string):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    result = embeddings.embed_documents(texts=string)\n",
    "    return result[0]\n",
    "\n",
    "# Provide the file path to your text file\n",
    "file_path = 'database.txt'\n",
    "\n",
    "# Read and separate the roles\n",
    "roles = read_and_separate_roles(file_path)\n",
    "\n",
    "role = roles[1].split(':')[0]\n",
    "print(role)\n",
    "embed = make_embeddings(role)\n",
    "len(embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine Learning Engineer:\\n    Programming Skills such as Python/R/Java\\n    Machine Learning Algorithms such as Supervised Learning, Unsupervised Learning, Reinforcement Learning\\n    Deep Learning Frameworks such as TensorFlow, PyTorch, Keras\\n    Cloud Platforms such as AWS, Microsoft Azure, Google Cloud\\n    Database Knowledge\\n    Statistics and Probability\\n    Libraries such as Scikit-learn, NumPy, Pandas\\n    Data Visualization Tools such as Matplotlib, Seaborn\\n    Algorithm Building and Optimization\\n    Model Evaluation and Optimization Techniques\\n    Confusion Matrix\\n    ROC-AUC\\n    F1 Score\\n    Hyperparameter Tuning Methods\\n    Precision-Recall Curve\\n    Grid Search\\n    Random Search',\n",
       " 'MLOps Engineer:\\n    Programming Skills such as Python/R/Java\\n    Machine Learning Familiarity\\n    Machine Learnign Algorithms\\n    DevOps Tools such as Docker, Kubernetes, Jenkins, Ansible\\n    Cloud Platforms such as AWS, Microsoft Azure, Google Cloud\\n    CI/CD Practices\\n    Database Knowledge\\n    SQL\\n    NoSQL\\n    Big Data Tools such as Hadoop, Spark\\n    Microservices and APIs\\n    Version Control\\n    Git\\n    Monitoring Tools\\n    Grafana\\n    Prometheus\\n    Project Management',\n",
       " 'ML Scientist/Researcher:\\n    Mathematical Skills including proficiency in Statistics, Probability, Linear Algebra, Calculus\\n    Programming Skills such as Python/R\\n    MATLAB\\n    Machine Learning and Deep Learning Algorithms\\n    Data Management and Transformation\\n    Data Cleaning\\n    Deep Learning Libraries such as TensorFlow, PyTorch\\n    Scientific Libraries such Scikit-Learn, Pandas, NumPy\\n    Research and Presentation Skills\\n    Cloud Platforms\\n    Algorithm Design and Implementation\\n    Model Evaluation and Optimization Techniques including Confusion Matrix, ROC-AUC, F1 Score, Hyperparameter Tuning Methods, Precision-Recall Curve, Grid Search, Random Search',\n",
       " 'Data Engineer:\\n    Programming Skills such as Python/Java/C++\\n    Hadoop-Based Technologies such as HDFS, MapReduce, Hive, Pig\\n    Data Pipelines and Architectures\\n    Big Data Tools including Spark, Kafka, Linux/UNIX Proficiency\\n    Relational Databases including MySQL, PostgreSQL\\n    NoSQL Databases such as MongoDB, Redis, Cassandra, Couchbase, HBase\\n    Stream Processing\\n    Apache Kafka\\n    Apache Storm\\n    Apache Samza\\n    Spark Streaming\\n    Flink Streaming\\n    Data Warehousing and ETL using Hive, Impala, Presto, Apache Drill, Amazon Redshift, Google BigQuery\\n    Data Orchestration using Apache Oozie, Apache Airflow, Luigi, Azkaban, Data Ingestion, Flume, Sqoop, Apache Nifi\\n    Data Integration using Apache Camel, Talend\\n    Cloud Services and Storage such as Amazon Web Services (AWS), Google Cloud Platform (GCP), Azure\\n    Cloud Storage knowledge such as S3 (AWS), EC2 (AWS), AWS Lambda (AWS)',\n",
       " 'Data Scientist:\\n    Mathematical Skills including proficiency in Statistics, Probability, Linear Algebra, Calculus\\n    Programming Skills such as Python/R/Java\\n    Machine Learning Algorithms such as Supervised Learning, Unsupervised Learning, Reinforcement Learning\\n    Data Visualization Tools such as Matplotlib, Seaborn\\n    SQL for Data Manipulation\\n    Big Data Tools such as Hadoop, Spark\\n    Deep Learning Frameworks such as TensorFlow, PyTorch\\n    Cloud Platforms such as AWS, Google Cloud, Microsoft Azure\\n    Scientific Computation Tools such sa NumPy, SciPy, Scikit-learn\\n    Data Preprocessing including Handling Balanced Datasets, Data Cleaning, Data Transformation, Data Augmentation',\n",
       " 'Big Data Engineer:\\n    Programming Skills such as Python/Java\\n    Big Data Programming Technologies such asHadoop, MapReduce, Spark\\n    Cloud Platforms such as AWS, Google Cloud, Microsoft Azure\\n    Data Warehousing and ETL\\n    SQL and NoSQL Databases\\n    Real-Time Processing Frameworks such as Kafka, Flink\\n    Linux/UNIX Proficiency\\n    Big Data File Formats such as Avro, Parquet\\n    Docker and Kubernetes\\n    Distributed Systems Concepts',\n",
       " 'Data Visualization Expert:\\n    Programming Skills such as Python/Java\\n    Data Visualization Tools such as Tableau, Power BI, QlikView\\n    Data Handling\\n    SQL\\n    Data Preprocessing using pandas, numpy\\n    Statistical Understanding\\n    Storytelling with Data\\n    Color Theory Knowledge\\n    Business Analytics']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(roles)):\n",
    "    role = roles[i].split(':')[0]\n",
    "    index.upsert([\n",
    "        (f'id-{i}', make_embeddings(roles[i]), {'role_title': role, 'description': roles[i]})\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(roles)):\n",
    "    index.delete(ids=[f'id-{i}'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'id-5',\n",
       "              'metadata': {'description': 'Big Data Engineer:\\n'\n",
       "                                          '    Programming Skills such as '\n",
       "                                          'Python/Java\\n'\n",
       "                                          '    Big Data Programming '\n",
       "                                          'Technologies such asHadoop, '\n",
       "                                          'MapReduce, Spark\\n'\n",
       "                                          '    Cloud Platforms such as AWS, '\n",
       "                                          'Google Cloud, Microsoft Azure\\n'\n",
       "                                          '    Data Warehousing and ETL\\n'\n",
       "                                          '    SQL and NoSQL Databases\\n'\n",
       "                                          '    Real-Time Processing Frameworks '\n",
       "                                          'such as Kafka, Flink\\n'\n",
       "                                          '    Linux/UNIX Proficiency\\n'\n",
       "                                          '    Big Data File Formats such as '\n",
       "                                          'Avro, Parquet\\n'\n",
       "                                          '    Docker and Kubernetes\\n'\n",
       "                                          '    Distributed Systems Concepts',\n",
       "                           'role_title': 'Big Data Engineer'},\n",
       "              'score': 0.952141941,\n",
       "              'values': []},\n",
       "             {'id': 'id-3',\n",
       "              'metadata': {'description': 'Data Engineer:\\n'\n",
       "                                          '    Programming Skills such as '\n",
       "                                          'Python/Java/C++\\n'\n",
       "                                          '    Hadoop-Based Technologies such '\n",
       "                                          'as HDFS, MapReduce, Hive, Pig\\n'\n",
       "                                          '    Data Pipelines and '\n",
       "                                          'Architectures\\n'\n",
       "                                          '    Big Data Tools including Spark, '\n",
       "                                          'Kafka, Linux/UNIX Proficiency\\n'\n",
       "                                          '    Relational Databases including '\n",
       "                                          'MySQL, PostgreSQL\\n'\n",
       "                                          '    NoSQL Databases such as '\n",
       "                                          'MongoDB, Redis, Cassandra, '\n",
       "                                          'Couchbase, HBase\\n'\n",
       "                                          '    Stream Processing\\n'\n",
       "                                          '    Apache Kafka\\n'\n",
       "                                          '    Apache Storm\\n'\n",
       "                                          '    Apache Samza\\n'\n",
       "                                          '    Spark Streaming\\n'\n",
       "                                          '    Flink Streaming\\n'\n",
       "                                          '    Data Warehousing and ETL using '\n",
       "                                          'Hive, Impala, Presto, Apache Drill, '\n",
       "                                          'Amazon Redshift, Google BigQuery\\n'\n",
       "                                          '    Data Orchestration using Apache '\n",
       "                                          'Oozie, Apache Airflow, Luigi, '\n",
       "                                          'Azkaban, Data Ingestion, Flume, '\n",
       "                                          'Sqoop, Apache Nifi\\n'\n",
       "                                          '    Data Integration using Apache '\n",
       "                                          'Camel, Talend\\n'\n",
       "                                          '    Cloud Services and Storage such '\n",
       "                                          'as Amazon Web Services (AWS), '\n",
       "                                          'Google Cloud Platform (GCP), Azure\\n'\n",
       "                                          '    Cloud Storage knowledge such as '\n",
       "                                          'S3 (AWS), EC2 (AWS), AWS Lambda '\n",
       "                                          '(AWS)',\n",
       "                           'role_title': 'Data Engineer'},\n",
       "              'score': 0.926981926,\n",
       "              'values': []}],\n",
       " 'namespace': ''}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"ABOUT THE ROLE:\n",
    "\n",
    "We are seeking a full-time Data Engineer III to join our team and define and deploy mission-critical data infrastructure. The primary responsibility for this role is defining the processing of large healthcare datasets which enable Rightway to generate insights that improve the health of more than 1 million members. Necessary skills for this role are expertise with the AWS ecosystem and proficiency in writing production-ready Python code. Exposure to medical data is a plus but not required.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WHAT YOU'LL DO:\n",
    "\n",
    "Write production-level Python and SQL code\n",
    "Use AWS services such as Lambda, Glue, and Fargate\n",
    "Use Apache Airflow to build batch data pipelines\n",
    "Use DBT to maintain Extract, Load, and Transform (ELT) processes\n",
    "Design database schemas in PostgreSQL to optimize data integrity and performance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WHO YOU ARE:\n",
    "\n",
    "4+ years of industry experience\n",
    "Experience using AWS systems in a production environment such as Lambda and Glue\n",
    "Have written SQL and Python code in a production environment\n",
    "Has experience scaling data infrastructure to handle high volumes of data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EXTRA CREDIT:\n",
    "\n",
    "Experience with medical and pharmacy claims data\n",
    "Experience deploying applications using serverless architecture\n",
    "Experience with Master Data Management and data governance\n",
    "Use infrastructure-as-code services such as Terraform or Cloudformation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BASE SALARY: $125,000-$175,000\n",
    "\n",
    "\"\"\"\n",
    "json_format = index.query(\n",
    "    make_embeddings(string=query),\n",
    "    top_k=2,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "json_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Data Engineer:\n",
      "    Programming Skills such as Python/Java\n",
      "    Big Data Programming Technologies such asHadoop, MapReduce, Spark\n",
      "    Cloud Platforms such as AWS, Google Cloud, Microsoft Azure\n",
      "    Data Warehousing and ETL\n",
      "    SQL and NoSQL Databases\n",
      "    Real-Time Processing Frameworks such as Kafka, Flink\n",
      "    Linux/UNIX Proficiency\n",
      "    Big Data File Formats such as Avro, Parquet\n",
      "    Docker and Kubernetes\n",
      "    Distributed Systems Concepts\n"
     ]
    }
   ],
   "source": [
    "context = json_format['matches'][0]['metadata']['description']\n",
    "print (context)\n",
    "seniority_level = 'Entry'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"unanswered_questions\": [\\n\"Do the candidates need to have experience with Big Data Programming Technologies such as Hadoop, MapReduce, and Spark?\",\\n\"Do the candidates need to be proficient with Cloud Platforms other than AWS, such as Google Cloud and Microsoft Azure?\",\\n\"Do the candidates need to have experience with Data Warehousing and ETL?\",\\n\"Do the candidates need to have experience with NoSQL Databases?\",\\n\"Do the candidates need to have experience with Real-Time Processing Frameworks such as Kafka and Flink?\",\\n\"Do the candidates need to have Linux/UNIX Proficiency?\",\\n\"Do the candidates need to have knowledge of Big Data File Formats such as Avro and Parquet?\",\\n\"Do the candidates need to have experience with Docker and Kubernetes?\",\\n\"Do the candidates need to have understanding of Distributed Systems Concepts?\"\\n]\\n}'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4', temperature=0.0)\n",
    "llm.predict(f'''\n",
    "Task: You are Mike and your role is to ask technical questions regarding the skills from the TEXT below\n",
    "TEXT: \n",
    "{context}\n",
    "Kindly check if the any of the skill is already discussed in the job desciption. Only unanswered questions needs to be asked\n",
    "job_description:\n",
    "{query}\n",
    "\n",
    "\n",
    "Question Format: <<(Do the candidate needs to have experience with xyz...)>>\n",
    "Output Format: JSON\n",
    "key: \"unanswered_questions\" value: [<<(for example: Do the candidate needs to have experience with xyz)>>, ....]\n",
    "\n",
    "Instructions:\n",
    "- Ensure to ask questions based on the seniority level\n",
    "- Ask the broad questions from the candidates in such a way that similar information is asked in one single question without dropping any single piece of information\n",
    "- Make sure to ask the questions in the interrogative tone\n",
    "- Make sure to ask the questions that are most relevant to the provided job descrition and role seniority level\n",
    "- if the information is given in the job description about something do not ask it again\n",
    "''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer = '{\\n\"unanswered_questions\": [\\n\"Do the candidates need to have experience with Big Data Programming Technologies such as Hadoop, MapReduce, and Spark?\",\\n\"Do the candidates need to be proficient with Cloud Platforms other than AWS, like Google Cloud and Microsoft Azure?\",\\n\"Do the candidates need to have experience with Data Warehousing and ETL?\",\\n\"Do the candidates need to have experience with NoSQL Databases, in addition to SQL?\",\\n\"Do the candidates need to have experience with Real-Time Processing Frameworks such as Kafka and Flink?\",\\n\"Do the candidates need to have Linux/UNIX Proficiency?\",\\n\"Do the candidates need to have knowledge of Big Data File Formats such as Avro and Parquet?\",\\n\"Do the candidates need to have experience with Docker and Kubernetes?\",\\n\"Do the candidates need to have understanding of Distributed Systems Concepts?\"\\n]\\n}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do the candidates need to have experience with Big Data Programming Technologies such as Hadoop, MapReduce, and Spark?', 'Do the candidates need to be proficient with Cloud Platforms other than AWS, like Google Cloud and Microsoft Azure?', 'Do the candidates need to have experience with Data Warehousing and ETL?', 'Do the candidates need to have experience with NoSQL Databases, in addition to SQL?', 'Do the candidates need to have experience with Real-Time Processing Frameworks such as Kafka and Flink?', 'Do the candidates need to have Linux/UNIX Proficiency?', 'Do the candidates need to have knowledge of Big Data File Formats such as Avro and Parquet?', 'Do the candidates need to have experience with Docker and Kubernetes?', 'Do the candidates need to have understanding of Distributed Systems Concepts?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "final = json.loads(answer)\n",
    "print (final['unanswered_questions'])\n",
    "len(final['unanswered_questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"precise_questions\": [\\'Do the candidates need to have experience with Big Data Programming Technologies such as Hadoop, MapReduce, and Spark?\\', \\'Do the candidates need to be proficient with Cloud Platforms other than AWS, like Google Cloud and Microsoft Azure?\\', \\'Do the candidates need to have experience with Data Warehousing and ETL?\\', \\'Do the candidates need to have Linux/UNIX Proficiency?\\', \\'Do the candidates need to have understanding of Distributed Systems Concepts?\\']\\n}'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(f'''Using the questions written below and the seniority level\n",
    "    <<{final['unanswered_questions']}>>\n",
    "    role_seniority_level:\n",
    "    <<{seniority_level}>>\n",
    "Reduce the number of questions to 5 based on the seniority level. Think Critically for that!. Make sure to return a dictionery with a key precise_questions''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JDAIB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
